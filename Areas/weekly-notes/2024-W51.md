## News

## Models

## Papers

## Tools

## Useful learning

## In Radar
- Real-time conversations with AI.

## Scratchpad

**Alibaba Cloud**Â releasedÂ _**Qwen2.5-Coder-32B**_, an open-source model for programming tasks that matches the coding capabilities of GPT-4o. In addition to this flagship model, four new models have been released, expanding the Qwen2.5-Coder family to a total of six models, ranging in sizes from 0.5B to 32B. AnÂ [Artifacts](https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts)Â app, similar to the Claude Artifacts, has also been launched [_[Details](https://qwenlm.github.io/blog/qwen2.5-coder-family/)_Â |Â _[Demo](https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo)_].

![  ](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec552495-ee3f-4679-896c-c42deff9836b_3376x1685.png)

https://www.anthropic.com/news/prompt-improver


https://stripe.dev/blog/adding-payments-to-your-agentic-workflows

https://opencoder-llm.github.io/
https://lmarena.ai/?leaderboard

https://web.lmarena.ai/

https://blog.langchain.dev/introducing-prompt-canvas/

https://blog.langchain.dev/promptim/

https://platform.openai.com/docs/guides/predicted-outputs


Go through all the docs of OpenAI and Anthropic to see what new features that they've released recently


https://mistral.ai/news/mistral-moderation/

**Hugging Face**Â research team releasedÂ _**SmolLM2**_, a family of open-source compact language models available in three size: 135M, 360M, and 1.7B parameters. SmolLM2 1.7B outperforms Qwen 2.5 1.5B & Llama 3.21B. The models can handle text rewriting, summarization and function calling [_[Details](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)_].

[  
](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19c86106-df2c-4e50-91ca-1f86730a7b5e_2138x1694.heic)

https://www.morphic.sh/

----

https://platform.openai.com/docs/guides/prompt-generation?context=text-out
https://github.blog/changelog/2024-12-17-github-models-introduces-ai-powered-system-prompt-enhancement-ga/

https://x.com/AshikNesin/status/1869683311641350218?t=5YR-WkcAUMX-ovkaQS0DVA&s=19


OpenAI o3 model is released

ðŸ”¥ Transformers.js v3.2 â€” Moonshine for real-time speech recognition, Phi-3.5 Vision for multi-frame image understanding and reasoning, and more!
https://github.com/huggingface/transformers.js/releases/tag/3.2.0

https://www.raymondcamden.com/2024/12/18/summarizing-with-transformersjs


https://www.aiengineerpack.com/

![[Pasted image 20241221161530.png]]
https://www.promptingguide.ai/

ðŸ’£ AI bypassed by creative spelling LINK
Anthropic's research reveals that advanced AI systems like GPT-4o and Claude 3.5 Sonnet can be tricked 89% and 78% of the time, respectively, using simple text variations that bypass their safety filters.
The "Best-of-N (BoN)" jailbreak technique involves creating different forms of restricted queries that confuse AI models by altering the text format without changing the underlying meaning.
Methods to outsmart AI defenses also apply to vision and audio systems, using tactics like changing text colors or speaking differently to bypass security measures.

https://arxiv.org/html/2412.03556v1


![[2024-12-22 at 11.07.41@2x.png]]
https://www.youtube.com/watch?v=2ZpK6BdEBNQ&ab_channel=bycloud

An introduction to preparing your own dataset for LLM training https://aws.amazon.com/blogs/machine-learning/an-introduction-to-preparing-your-own-dataset-for-llm-training/
The Future of Data Engineering: DEW's 2025 Predictions https://www.dataengineeringweekly.com/p/the-future-of-data-engineering-dews
